# Data Model: Basic LLM Agent

**Feature**: Basic LLM Agent  
**Date**: 2025-01-27  
**Purpose**: Define core data structures and entities for LLM agent functionality

## Core Entities

### Agent

Represents an LLM-powered agent with identity, configuration, instructions, and tools.

**Fields**:
- `name :: Text` - Unique identifier for the agent (required)
- `description :: Text` - Description of agent capabilities (optional, recommended for multi-agent)
- `model :: Model` - LLM model identifier (e.g., "gpt-4", "gpt-3.5-turbo") (required)
- `instruction :: Text` - Instructions that guide agent behavior, personality, constraints, and tool usage (required)
- `tools :: [Tool]` - List of tools available to the agent (optional, empty list for conversational agents)
- `generateContentConfig :: Maybe GenerateContentConfig` - Optional LLM generation configuration (temperature, max tokens, etc.)

**Validation Rules**:
- `name` must be non-empty
- `name` must be unique within agent scope (enforced by runtime, not type system)
- `model` must be a valid model identifier for the configured LLM provider
- `instruction` must be non-empty
- `tools` list can be empty (conversational agent) or contain one or more tools

**Relationships**:
- Agent has zero or more Tools
- Agent uses one Model
- Agent maintains ConversationContext during execution

**Type Definition**:
```haskell
data Agent = Agent
  { agentName :: Text
  , agentDescription :: Maybe Text
  , agentModel :: Model
  , agentInstruction :: Text
  , agentTools :: [Tool]
  , agentGenerateContentConfig :: Maybe GenerateContentConfig
  }
  deriving (Eq, Show, Generic)
```

### Model

Represents an LLM model identifier and provider configuration.

**Fields**:
- `modelId :: Text` - Model identifier string (e.g., "gpt-4", "gpt-3.5-turbo")
- `provider :: LLMProvider` - Provider type (OpenAI, Anthropic, Google, etc.)

**Validation Rules**:
- `modelId` must be non-empty
- `modelId` must be valid for the specified provider

**Type Definition**:
```haskell
data Model = Model
  { modelId :: Text
  , modelProvider :: LLMProvider
  }
  deriving (Eq, Show)

data LLMProvider
  = OpenAI
  | Anthropic
  | Google
  deriving (Eq, Show)
```

### Tool

Represents a capability or function that extends agent abilities beyond LLM's built-in knowledge.

**Fields**:
- `toolName :: Text` - Unique name for the tool (required)
- `toolDescription :: Text` - Description of what the tool does (required)
- `toolSchema :: Value` - JSON schema describing tool parameters (required)
- `toolInvoke :: Value -> IO Value` - Function that invokes the tool with JSON parameters and returns JSON result (required)

**Validation Rules**:
- `toolName` must be non-empty
- `toolName` must be unique within an agent's tool list
- `toolDescription` must be non-empty
- `toolSchema` must be valid JSON schema
- `toolInvoke` function must handle JSON parameter conversion and error cases

**Relationships**:
- Tool belongs to zero or more Agents (tools can be shared)
- Tool invocation produces ToolResult

**Type Definition**:
```haskell
data Tool = Tool
  { toolName :: Text
  , toolDescription :: Text
  , toolSchema :: Value  -- Aeson Value for JSON schema
  , toolInvoke :: Value -> IO Value  -- JSON in, JSON out
  }
  deriving (Generic)  -- Note: Cannot derive Eq/Show due to function field
```

### Message

Represents a single message in a conversation.

**Fields**:
- `messageRole :: MessageRole` - Role of the message sender (user or assistant)
- `messageContent :: Text` - Text content of the message (required)

**Validation Rules**:
- `messageContent` must be non-empty

**Relationships**:
- Message belongs to ConversationContext

**Type Definition**:
```haskell
data MessageRole
  = UserRole
  | AssistantRole
  deriving (Eq, Show)

data Message = Message
  { messageRole :: MessageRole
  , messageContent :: Text
  }
  deriving (Eq, Show)
```

### ConversationContext

Represents the history of messages in a conversation.

**Fields**:
- `contextMessages :: [Message]` - Ordered list of messages (most recent last)

**Validation Rules**:
- Messages should alternate between user and assistant (not strictly enforced, but expected pattern)
- Context can be empty (new conversation)

**Relationships**:
- ConversationContext contains zero or more Messages
- ConversationContext is maintained by Agent during execution

**Type Definition**:
```haskell
type ConversationContext = [Message]
```

### AgentResponse

Represents the response generated by an agent.

**Fields**:
- `responseContent :: Text` - Text content of the agent's response (required)
- `responseToolsUsed :: [ToolInvocation]` - List of tools invoked during response generation (optional)

**Validation Rules**:
- `responseContent` must be non-empty (unless error case)

**Relationships**:
- AgentResponse is produced by Agent execution
- AgentResponse may reference ToolInvocation results

**Type Definition**:
```haskell
data AgentResponse = AgentResponse
  { responseContent :: Text
  , responseToolsUsed :: [ToolInvocation]
  }
  deriving (Eq, Show)
```

### ToolInvocation

Represents a single tool invocation during agent execution.

**Fields**:
- `invocationToolName :: Text` - Name of the tool that was invoked
- `invocationArgs :: Value` - JSON arguments passed to the tool
- `invocationResult :: Either Text Value` - Tool result (Right) or error message (Left)

**Validation Rules**:
- `invocationToolName` must match a tool in the agent's tool list
- `invocationArgs` must conform to tool's schema

**Relationships**:
- ToolInvocation references a Tool
- ToolInvocation is part of AgentResponse

**Type Definition**:
```haskell
data ToolInvocation = ToolInvocation
  { invocationToolName :: Text
  , invocationArgs :: Value
  , invocationResult :: Either Text Value
  }
  deriving (Eq, Show)
```

### GenerateContentConfig

Represents optional LLM generation configuration parameters.

**Fields**:
- `temperature :: Maybe Double` - Sampling temperature (0.0 to 2.0, typically 0.0 to 1.0)
- `maxTokens :: Maybe Int` - Maximum tokens in response
- `topP :: Maybe Double` - Nucleus sampling parameter
- `topK :: Maybe Int` - Top-K sampling parameter

**Validation Rules**:
- `temperature` must be in valid range (0.0 to 2.0) if present
- `maxTokens` must be positive if present
- `topP` must be in valid range (0.0 to 1.0) if present
- `topK` must be positive if present

**Type Definition**:
```haskell
data GenerateContentConfig = GenerateContentConfig
  { configTemperature :: Maybe Double
  , configMaxTokens :: Maybe Int
  , configTopP :: Maybe Double
  , configTopK :: Maybe Int
  }
  deriving (Eq, Show)
```

## State Transitions

### Agent Execution Flow

1. **Agent Creation**: Agent created with name, description, model, instruction, and optional tools
2. **Agent Execution**: Agent receives user input, processes with LLM, may invoke tools, generates response
3. **Context Update**: Conversation context updated with user message and agent response
4. **Response Return**: AgentResponse returned to developer

### Tool Invocation Flow

1. **Tool Selection**: Agent (via LLM) selects appropriate tool based on user input and instructions
2. **Parameter Extraction**: LLM provides tool parameters as JSON
3. **Schema Validation**: Parameters validated against tool schema
4. **Tool Execution**: Tool invocation function called with validated parameters
5. **Result Handling**: Tool result (or error) returned to agent
6. **Response Generation**: Agent incorporates tool result into final response

## Notes

- All text fields use `Text` type (not `String`) for efficiency
- JSON handling uses `aeson` `Value` type for flexibility
- Tool invocation uses `IO` for side effects (tool execution may have side effects)
- Error handling uses `Either Text` for recoverable errors
- Type safety prioritized while maintaining flexibility for LLM integration
